{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "# DL\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "import evaluate\n",
    "import torch\n",
    "\n",
    "\n",
    "# from transformers import TrainingArguments, Trainer\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "# visualization, data utils\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import datasets\n",
    "from PIL import Image\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import wandb\n",
    "\n",
    "\n",
    "# evaluation\n",
    "import evaluate, sklearn\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# other utils\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "import os, glob, sys, shutil, datetime\n",
    "from pathlib import Path\n",
    "import random, math\n",
    "from tqdm import tqdm\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, Tuple, Union, List, Optional, Callable, Iterable\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "import gc\n",
    "from easydict import EasyDict as edict\n",
    "import IPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ecbf615ce8744db8d4b39c5a7be8170",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6777729fffa46d4b553aeadf3304f75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/5100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7128cdfa49334abea7fedf1b46c1b024",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1050 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7f0032a1a93484e8b1bf115fa384b7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/1050 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datasets\n",
    "import underthesea\n",
    "from pyvi import ViTokenizer\n",
    "from spacy.lang.vi import STOP_WORDS as VIETNAMESE_STOP_WORDS\n",
    "\n",
    "from src.data.vietnamese_eda import VietnameseEDATransform\n",
    "EDA_TRANSFORM = VietnameseEDATransform()\n",
    "\n",
    "\n",
    "# def text_preprocess(text):\n",
    "#     # normalize text\n",
    "#     text = underthesea.text_normalize(text)\n",
    "#     # word segmentation\n",
    "#     text = ViTokenizer.tokenize(text.lower())\n",
    "#     # remove stop words\n",
    "#     words = text.split()\n",
    "#     words = [word for word in words if word not in VIETNAMESE_STOP_WORDS]\n",
    "#     return \" \".join(words)\n",
    "\n",
    "\n",
    "def read_data(file_path, is_train=True):\n",
    "    # remove header, two columns with name 'Class' and 'Data'\n",
    "    data = pd.read_csv(file_path, delimiter=\"\\t\", header=None, names=[\"label\", \"text\"])\n",
    "    data = data[1:]\n",
    "    data = datasets.Dataset.from_pandas(data)\n",
    "    if is_train:\n",
    "        # transform: just get a random EDA transform if exists, else original text\n",
    "        transform = lambda list_text: [random.choice(EDA_TRANSFORM(x) or [x]) for x in list_text] \n",
    "    else:\n",
    "        transform = lambda x: x\n",
    "    data = data.map(lambda x: {\"label\": int(x[\"label\"]) + 1, \"text\": (x[\"text\"])})\n",
    "    # data.set_transform(lambda x: {\"label\": x[\"label\"], \"text\": transform(x[\"text\"])})\n",
    "    data = data.cast_column(\n",
    "        \"label\",\n",
    "        datasets.ClassLabel(num_classes=3, names=[\"negative\", \"neutral\", \"positive\"]),\n",
    "    )\n",
    "    return data\n",
    "\n",
    "def get_transform(is_train):\n",
    "    if is_train:\n",
    "        aug_transform = lambda list_text: [random.choice(EDA_TRANSFORM(x)) for x in list_text] \n",
    "    else:\n",
    "        aug_transform = lambda x: x\n",
    "    return lambda x: {\"label\": (x[\"label\"]), \"text\": aug_transform(x[\"text\"])}\n",
    "\n",
    "train_table = read_data(\"vlsp_sentiment_train.csv\")\n",
    "# split train_table to train and validation\n",
    "train_val_dict = train_table.train_test_split(\n",
    "    test_size=0.1, stratify_by_column=\"label\", seed=44\n",
    ")\n",
    "train_table = train_val_dict[\"train\"]\n",
    "val_table = train_val_dict[\"test\"]\n",
    "\n",
    "test_table = read_data(\"vlsp_sentiment_test.csv\")\n",
    "\n",
    "train_table.set_transform(get_transform(is_train=True))\n",
    "val_table.set_transform(get_transform(is_train=False))\n",
    "test_table.set_transform(get_transform(is_train=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of matrix word embedding: (1587507, 300)\n",
      "Vocab size: 1587507\n",
      "Vector dimension: 300\n"
     ]
    }
   ],
   "source": [
    "from src.models.phow2vec import PhoW2VecWrapper\n",
    "\n",
    "w2v_model = PhoW2VecWrapper(max_length=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method based on CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self, word2vec_model, input_dim, num_filters, filter_sizes, output_dim, dropout\n",
    "    ):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        self.embedding = word2vec_model\n",
    "\n",
    "        self.convs = nn.ModuleList(\n",
    "            [\n",
    "                nn.Conv1d(\n",
    "                    in_channels=input_dim, out_channels=num_filters, kernel_size=fs\n",
    "                )\n",
    "                for fs in filter_sizes\n",
    "            ]\n",
    "        )\n",
    "        self.max_pools = nn.ModuleList(\n",
    "            nn.AdaptiveMaxPool1d(output_size=1) for _ in filter_sizes\n",
    "        )  # make (B, C, L) to (B, C, 1), use adaptive for not caring about the length of the input\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(len(filter_sizes) * num_filters, output_dim)\n",
    "        self.loss_fct = nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "\n",
    "    def init_weights(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if \"weight\" in name:\n",
    "                nn.init.normal_(param.data, mean=0, std=0.02)\n",
    "            elif \"bias\" in name:\n",
    "                nn.init.constant_(param.data, 0)\n",
    "\n",
    "    def forward(self, texts, labels=None):\n",
    "        # x is text or len of text\n",
    "        # this is weird because I combined model and tokenizer into 1\n",
    "        x = self.embedding(texts)  # [batch_size, sent_len, emb_dim]\n",
    "        x = x.transpose(-2, -1)  # [batch_size, emb_dim, sent_len]\n",
    "        x = x.to(self.device)\n",
    "        # consider emb_dim as input channel\n",
    "        conved_output = [\n",
    "            F.relu(conv(x)) for conv in self.convs\n",
    "        ]  # list of tensor shaped [batch_size, num_filter, sent_len - filter_sizes[n] + 1]\n",
    "        pooled_output = [\n",
    "            pool(conv).squeeze(-1) for conv, pool in zip(conved_output, self.max_pools)\n",
    "        ]  # list of tensor shaped [batch_size, num_filter]\n",
    "        cat = torch.cat(\n",
    "            pooled_output, dim=-1\n",
    "        )  # [batch_size, num_filter * len(filter_sizes)]\n",
    "        drop_output = self.dropout(cat)  # [batch_size, num_filter * len(filter_sizes)]\n",
    "        logits = self.fc(drop_output)  # [batch_size, output_dim]\n",
    "\n",
    "        # output should be wrapped in edict, for multi-way attribute-accessing\n",
    "        return_dict = edict({\"logits\": logits})  # logits will use in inference\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits, labels)\n",
    "            return_dict.losses = {\"ce_loss\": loss}  # losses will use in training\n",
    "\n",
    "        return edict(return_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'logits': tensor([[-0.0295, -0.0078,  0.0065],\n",
       "         [-0.0256, -0.0057,  0.0155],\n",
       "         [-0.0211, -0.0046,  0.0092]], grad_fn=<AddmmBackward0>),\n",
       " 'losses': {'ce_loss': tensor(1.0998, grad_fn=<NllLossBackward0>)}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_model = CNNClassifier(\n",
    "    word2vec_model=w2v_model,\n",
    "    input_dim=300,\n",
    "    num_filters=100,\n",
    "    filter_sizes=[3, 4, 5],\n",
    "    output_dim=3,\n",
    "    dropout=0.1,\n",
    ")\n",
    "\n",
    "cnn_model(\n",
    "    [\"Tôi là sinh viên trường đại học bách khoa hà nội\"] * 3, torch.tensor([1, 0, 2])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method based on LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import dropout_\n",
    "\n",
    "\n",
    "class LSTMClassifier(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        word2vec_model,\n",
    "        input_dim=300,\n",
    "        hidden_dims=[384, 384, 384],\n",
    "        output_dim=3,\n",
    "        n_layers=3,\n",
    "        bidirectional=True,\n",
    "        dropout=0.2,\n",
    "    ):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.embedding = word2vec_model\n",
    "\n",
    "        num_direct = 2 if bidirectional else 1\n",
    "        # hidden_dims is vector dim of single direction, output_dim of lstm is hidden_dim * num_direct\n",
    "\n",
    "        list_in_lstm_dims = [input_dim] + [\n",
    "            hidden_dims[i] * num_direct for i in range(len(hidden_dims) - 1)\n",
    "        ]\n",
    "        list_out_lstm_dims = hidden_dims\n",
    "\n",
    "        self.lstm_chain = nn.ModuleList(\n",
    "            [\n",
    "                nn.LSTM(\n",
    "                    input_size=list_in_lstm_dims[i],\n",
    "                    hidden_size=list_out_lstm_dims[i],\n",
    "                    num_layers=1,\n",
    "                    bidirectional=bidirectional,\n",
    "                    dropout=dropout,\n",
    "                    batch_first=True,\n",
    "                )\n",
    "                for i in range(n_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.max_pooling = nn.AdaptiveMaxPool1d(output_size=1)\n",
    "        self.fc = nn.Linear(list_out_lstm_dims[-1] * num_direct, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.loss_fct = nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "        self.init_weights()\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "\n",
    "    def init_weights(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if \"weight\" in name:\n",
    "                nn.init.normal_(param.data, mean=0, std=0.02)\n",
    "            elif \"bias\" in name:\n",
    "                nn.init.constant_(param.data, 0)\n",
    "\n",
    "    def forward(self, texts, labels=None):\n",
    "        x = self.embedding(texts)\n",
    "        x = x.to(self.device)  # [batch_size, sent_len, input_dim]\n",
    "\n",
    "        for lstm in self.lstm_chain:\n",
    "            x, (hidden, cell) = lstm(x)\n",
    "\n",
    "        # num_direct = 2 if bidirectional else 1\n",
    "        # x: [batch_size, sent_len, hidden_dim * num_direct] # last layer 's output for whole sequence\n",
    "        # hidden: [num_layer * num_direct, batch_size, hidden_dim] # last hidden state of all layers\n",
    "        # cell: [num_layer * num_direct, batch_size, hidden_dim] # last cell state of all layers\n",
    "\n",
    "        # take the last hidden state of the last layer as global feature\n",
    "        # the input have been padded to the right, so we can take the last hidden state as global feature\n",
    "        global_feature = x[:, -1, :]  # [batch_size, hidden_dim * num_direct]\n",
    "\n",
    "        dropout_output = self.dropout(\n",
    "            global_feature\n",
    "        )  # [batch_size, hidden_dim * num_direct]\n",
    "        logits = self.fc(dropout_output)\n",
    "        return_dict = edict({\"logits\": logits})\n",
    "        if labels is not None:\n",
    "            loss = self.loss_fct(logits, labels)\n",
    "            return_dict.losses = {\"ce_loss\": loss}\n",
    "        return edict(return_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid method combining CNN and LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dscilab_dungvo/workspace/bin/lib/python3.8/site-packages/torch/nn/modules/rnn.py:88: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'logits': tensor([[ 0.0522, -0.0197,  0.0052],\n",
       "         [ 0.0315,  0.0051,  0.0036],\n",
       "         [ 0.0454, -0.0192, -0.0017]], grad_fn=<AddmmBackward0>),\n",
       " 'losses': {'ce_loss': tensor(1.1069, grad_fn=<NllLossBackward0>)}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class HybridClassifer(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        word2vec_model,\n",
    "        input_dim=300,\n",
    "        lstm_hidden_dim=384,\n",
    "        dropout=0.2,\n",
    "        cnn_num_filters=300,\n",
    "        cnn_filter_sizes=[3, 4, 5],\n",
    "    ):\n",
    "        super(HybridClassifer, self).__init__()\n",
    "        self.embedding = word2vec_model\n",
    "\n",
    "        # just one layer as in slide for speed up\n",
    "        self.lstm = torch.nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=lstm_hidden_dim,\n",
    "            num_layers=1,\n",
    "            bidirectional=True,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.batch_norm = nn.BatchNorm1d(lstm_hidden_dim * 2)\n",
    "        self.convs = nn.ModuleList(\n",
    "            [\n",
    "                nn.Conv1d(\n",
    "                    in_channels=lstm_hidden_dim * 2,\n",
    "                    out_channels=cnn_num_filters,\n",
    "                    kernel_size=fs,\n",
    "                )\n",
    "                for fs in cnn_filter_sizes\n",
    "            ]\n",
    "        )\n",
    "        self.max_pools = nn.ModuleList(\n",
    "            nn.AdaptiveMaxPool1d(output_size=1) for _ in cnn_filter_sizes\n",
    "        )\n",
    "        self.fc = nn.Linear(cnn_num_filters * len(cnn_filter_sizes), 3)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.loss_fct = nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, texts, labels=None):\n",
    "        x = self.embedding(texts)\n",
    "        x = x.to(self.device)\n",
    "\n",
    "        # LSTM\n",
    "        x, (hidden, cell) = self.lstm(x)\n",
    "        # x: [batch_size, sent_len, hidden_dim * num_direct] # last layer 's output for whole sequence\n",
    "        x = x.transpose(\n",
    "            1, 2\n",
    "        )  # Transpose to (batch_size, hidden_dim * num_direct, sent_len)\n",
    "        x = self.batch_norm(x)\n",
    "        x = x.transpose(\n",
    "            1, 2\n",
    "        )  # Transpose back to (batch_size, sent_len, hidden_dim * num_direct)\n",
    "\n",
    "        # CNN\n",
    "        x = x.transpose(-2, -1)\n",
    "        conved_output = [F.relu(conv(x)) for conv in self.convs]\n",
    "        pooled_output = [\n",
    "            pool(conv).squeeze(-1) for conv, pool in zip(conved_output, self.max_pools)\n",
    "        ]\n",
    "        cat = torch.cat(pooled_output, dim=-1)\n",
    "        drop_output = self.dropout(cat)\n",
    "        logits = self.fc(drop_output)\n",
    "        return_dict = edict({\"logits\": logits})\n",
    "        if labels is not None:\n",
    "            loss = self.loss_fct(logits, labels)\n",
    "            return_dict.losses = {\"ce_loss\": loss}\n",
    "        return edict(return_dict)\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "\n",
    "    def init_weights(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if \"weight\" in name:\n",
    "                nn.init.normal_(param.data, mean=0, std=0.02)\n",
    "            elif \"bias\" in name:\n",
    "                nn.init.constant_(param.data, 0)\n",
    "\n",
    "\n",
    "hybrid_model = HybridClassifer(\n",
    "    word2vec_model=w2v_model,\n",
    "    input_dim=300,\n",
    "    lstm_hidden_dim=384,\n",
    "    dropout=0.2,\n",
    "    cnn_num_filters=300,\n",
    "    cnn_filter_sizes=[3, 4, 5, 6, 7],\n",
    ")\n",
    "\n",
    "\n",
    "hybrid_model(\n",
    "    [\"Tôi là sinh viên trường đại học bách khoa hà nội\"] * 3, torch.tensor([1, 0, 2])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dscilab_dungvo/workspace/bin/lib/python3.8/site-packages/torch/nn/modules/rnn.py:88: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.4 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'logits': tensor([[0.0003, 0.0003, 0.0003],\n",
       "         [0.0010, 0.0004, 0.0001],\n",
       "         [0.0005, 0.0005, 0.0002]], grad_fn=<AddmmBackward0>),\n",
       " 'losses': {'ce_loss': tensor(1.0985, grad_fn=<NllLossBackward0>)}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model = LSTMClassifier(\n",
    "    word2vec_model=w2v_model,\n",
    "    input_dim=300,\n",
    "    hidden_dims=[384, 512, 512],\n",
    "    output_dim=3,\n",
    "    n_layers=3,\n",
    "    bidirectional=True,\n",
    "    dropout=0.4,\n",
    ")\n",
    "\n",
    "lstm_model(\n",
    "    [\"Tôi là sinh viên trường đại học bách khoa hà nội\"] * 3, torch.tensor([1, 0, 2])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method based on Encoder-only Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import underthesea, pyvi\n",
    "\n",
    "\n",
    "class VietnameseTextPreprocessor(nn.Module):\n",
    "    def __init__(self, max_length=100):\n",
    "        super(VietnameseTextPreprocessor, self).__init__()\n",
    "        self.max_length = max_length\n",
    "        self.tokenizer = PhoW2VecWrapper(max_length=max_length)\n",
    "\n",
    "    def forward(self, texts):\n",
    "        return self.tokenizer(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoBackbone, AutoModelForPreTraining, AutoModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from easydict import EasyDict as edict\n",
    "\n",
    "\n",
    "class HuggingFaceModelWrapper(torch.nn.Module):\n",
    "    def __init__(self, model_name, num_classes=3, max_length=100):\n",
    "        super(HuggingFaceModelWrapper, self).__init__()\n",
    "\n",
    "        self.model_name = model_name\n",
    "        self.num_classes = num_classes\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "        self.backbone = AutoModel.from_pretrained(model_name)\n",
    "        self.pooling = lambda x: x[\n",
    "            :, 0, :\n",
    "        ]  # just use the first token as global feature\n",
    "\n",
    "        # my favourite MLP: linear, layer norm, gelu activation, linear\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(\n",
    "                self.backbone.config.hidden_size, self.backbone.config.hidden_size\n",
    "            ),\n",
    "            nn.LayerNorm(self.backbone.config.hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(self.backbone.config.hidden_size, num_classes),\n",
    "        )\n",
    "\n",
    "        self.loss_fct = nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "\n",
    "    def forward(self, texts, labels=None):\n",
    "        inputs = self.tokenizer(\n",
    "            texts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "        ).to(self.device)\n",
    "        outputs = self.backbone(**inputs)\n",
    "        global_feature = self.pooling(outputs.last_hidden_state)\n",
    "        logits = self.classifier(global_feature)\n",
    "\n",
    "        return_dict = edict({\"logits\": logits})  # will be used in inference\n",
    "        if labels is not None:\n",
    "            loss = self.loss_fct(logits, labels)\n",
    "            return_dict.losses = {\"ce_loss\": loss}  # will be used in training\n",
    "        return edict(return_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at vinai/phobert-base-v2 and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'logits': tensor([[-0.0735, -0.5013, -0.6271],\n",
       "         [-0.2695, -0.4358, -0.3927]], grad_fn=<AddmmBackward0>)}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# AutoModelForPreTraining.from_pretrained(\"vinai/phobert-base-v2\")\n",
    "wrapper_model = HuggingFaceModelWrapper(\"vinai/phobert-base-v2\", 3)\n",
    "\n",
    "\n",
    "wrapper_model(\n",
    "    [\"hello kitty\", \"tôi là sinh viên trường đại học bách khoa hà nội\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_collator_for_cnn(list_examples):\n",
    "    # list_examples: list of examples, each example is a dict\n",
    "    # list_examples[0] = {\"label\": tensor, \"text\": text}\n",
    "    # ...\n",
    "    # list_examples[n] = {\"label\": tensor, \"text\": text}\n",
    "    # return: dict of batched examples\n",
    "    labels = torch.Tensor([example[\"label\"] for example in list_examples]).long()\n",
    "\n",
    "    # weird implementation that combine model and tokenizer into 1\n",
    "    texts = [example[\"text\"] for example in list_examples]\n",
    "    return {\"labels\": labels, \"texts\": texts}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(model, dataloader):\n",
    "    \"\"\"Function calculates accuracy of given model on dataloader\n",
    "\n",
    "    Args:\n",
    "        model (CLIPClassifier): CLIP classifier model\n",
    "        dataloader (DataLoader): evaluation dataloader\n",
    "\n",
    "    Returns:\n",
    "        float: model's accuracy\n",
    "    \"\"\"\n",
    "    # create metric computer\n",
    "    accuracy_metric = evaluate.load(\"accuracy\")\n",
    "    f1_metric = evaluate.load(\"f1\")\n",
    "    precision_metric = evaluate.load(\"precision\")\n",
    "    recall_metric = evaluate.load(\"recall\")\n",
    "    list_metrics = [accuracy_metric, f1_metric, precision_metric, recall_metric]\n",
    "    # evaluate model\n",
    "    predictions_list = []\n",
    "    references_list = []\n",
    "    device = model.device\n",
    "\n",
    "    for batch in tqdm(\n",
    "        dataloader, total=len(dataloader), desc=\"Evaluate model on dataset\"\n",
    "    ):\n",
    "        batch[\"labels\"] = batch[\"labels\"].to(device)\n",
    "        predictions = model(**batch)[\"logits\"]\n",
    "        predictions_list.append(torch.argmax(predictions, dim=1))\n",
    "        references_list.append(batch[\"labels\"])\n",
    "\n",
    "    results_dict = {}\n",
    "    for metric in list_metrics:\n",
    "        if metric.name == \"accuracy\":\n",
    "            result_dict = metric.compute(\n",
    "                predictions=torch.concat(predictions_list),\n",
    "                references=torch.concat(references_list),\n",
    "            )\n",
    "        else:\n",
    "            result_dict = metric.compute(\n",
    "                predictions=torch.concat(predictions_list),\n",
    "                references=torch.concat(references_list),\n",
    "                average=\"macro\",\n",
    "            )\n",
    "        results_dict.update(result_dict)\n",
    "\n",
    "    # rename f1 to macro_f1\n",
    "    results_dict[\"macro_f1\"] = results_dict.pop(\"f1\")\n",
    "    results_dict[\"macro_precision\"] = results_dict.pop(\"precision\")\n",
    "    results_dict[\"macro_recall\"] = results_dict.pop(\"recall\")\n",
    "\n",
    "    return results_dict\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def classification_evaluate(\n",
    "    model: nn.Module,\n",
    "    dataset: datasets.Dataset,\n",
    "    tokenizer: None,\n",
    "    collate_fn: Callable,\n",
    "    batch_size: int = 64,\n",
    "    num_workers: int = 2,\n",
    "    device: str = \"cuda\",\n",
    "):\n",
    "\n",
    "    classifier = model\n",
    "    test_dataloader = DataLoader(\n",
    "        dataset, batch_size=batch_size, num_workers=num_workers, collate_fn=collate_fn\n",
    "    )\n",
    "    classifier = classifier.to(device)\n",
    "    results_dict = calculate_accuracy(classifier, test_dataloader)\n",
    "    print(f\"Evaluate metrics: {results_dict}\")\n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTrainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: DictConfig,\n",
    "        model: torch.nn.Module,\n",
    "        train_dataset: Union[torch.utils.data.Dataset, datasets.Dataset],\n",
    "        val_dataset: Union[torch.utils.data.Dataset, datasets.Dataset],\n",
    "        tokenizer: Optional[transformers.PreTrainedTokenizer] = None,\n",
    "        # labels: list[str] = [\"negative\", \"neutral\", \"positive\"],\n",
    "        optimizer: Optional[torch.optim.Optimizer] = None,\n",
    "        scheduler: Optional[torch.optim.lr_scheduler._LRScheduler] = None,\n",
    "        logger: Optional[wandb.sdk.wandb_run.Run] = None,\n",
    "        metrics_to_save_best: Optional[List[str]] = [\"val/metrics/accuracy\"],\n",
    "        device: Optional[torch.device] = \"cuda\",\n",
    "        collate_fn: Optional[Callable] = None,\n",
    "    ):\n",
    "\n",
    "        self.config = config\n",
    "        self.device = device\n",
    "        self._logger = logger\n",
    "        self.setup_output_dir()\n",
    "        # main components\n",
    "        self.model = model\n",
    "        self.train_dataset = train_dataset\n",
    "        self.val_dataset = val_dataset\n",
    "        # initialize some variables for training loop\n",
    "        self.cur_step: int = -1\n",
    "        self.cur_epoch: int = -1\n",
    "        self.exit_by_patience: bool = False\n",
    "        self.current_patience: int = -1\n",
    "        self.max_patience: int = config.get(\"patience\", math.inf)\n",
    "        self.best_metrics_values: Dict[str, Any] = {\n",
    "            **{key: -1 for key in metrics_to_save_best}\n",
    "        }\n",
    "        self.history_metrics: List[Dict[str, Any]] = []\n",
    "\n",
    "        self.collate_fn = collate_fn\n",
    "        self.tokenizer = tokenizer\n",
    "        self.build_dataloader(train_dataset)\n",
    "        # self.setup_optimizers_before_training(config)\n",
    "        print(self.config)\n",
    "\n",
    "    def setup_optimizers_before_training(self, config):\n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=config.get(\"lr\", 1e-4),\n",
    "            betas=config.get(\"betas\", (0.9, 0.995)),\n",
    "        )\n",
    "        self.scheduler = transformers.get_cosine_schedule_with_warmup(\n",
    "            self.optimizer,\n",
    "            num_warmup_steps=config.get(\"num_warmup_steps\", 100),\n",
    "            num_training_steps=config.get(\"max_epochs\", 10)\n",
    "            * len(self.train_dataloader),\n",
    "            num_cycles=config.get(\"num_cycles\", 0.5),\n",
    "            last_epoch=self.cur_epoch,\n",
    "        )\n",
    "\n",
    "    def build_dataloader(self, dataset):\n",
    "        self.train_dataloader = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=self.config.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.config.num_workers,\n",
    "            collate_fn=self.collate_fn,\n",
    "        )\n",
    "        self.val_dataloader = DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.config.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.config.num_workers,\n",
    "            collate_fn=self.collate_fn,\n",
    "        )\n",
    "\n",
    "    def setup_output_dir(self):\n",
    "        self.project_name = self._logger.project\n",
    "        self.group_name = self._logger.group\n",
    "        self.experiment_name = self._logger.name\n",
    "        if self._logger is None:\n",
    "            return\n",
    "        output_dir = self.config.get(\"output_dir\", \"checkpoints\")\n",
    "        prefix = (\n",
    "            f\"{output_dir}/{self.project_name}/{self.group_name}/{self.experiment_name}\"\n",
    "        )\n",
    "        if not os.path.exists(prefix):\n",
    "            os.makedirs(prefix)\n",
    "        with open(f\"{prefix}/config.yaml\", \"w\") as f:\n",
    "            OmegaConf.save(self.config, f)\n",
    "        self.checkpoint_prefix = prefix\n",
    "\n",
    "    def extract_loss(self, output, validation=False) -> torch.Tensor:\n",
    "        if hasattr(output, \"losses\"):\n",
    "            losses = output.losses\n",
    "        elif isinstance(output, torch.Tensor):\n",
    "            losses = {\"total\": output}\n",
    "        else:\n",
    "            losses = output\n",
    "        total_loss = 0\n",
    "        for key in losses:\n",
    "            if losses[key] is not None:\n",
    "                total_loss += losses[key]\n",
    "                loss_reduce = losses[key].detach()\n",
    "                if validation:\n",
    "                    mode = \"validation\"\n",
    "                else:\n",
    "                    mode = \"train\"\n",
    "                self.log(\n",
    "                    f\"{mode}/losses/{key}\",\n",
    "                    loss_reduce.item(),\n",
    "                )\n",
    "                # print(f\"{mode}/losses/{key}\", loss_reduce.item())\n",
    "        return total_loss\n",
    "\n",
    "    def train(self) -> None:\n",
    "        self.setup_optimizers_before_training(self.config)\n",
    "        model, optimizer, scheduler = (self.model, self.optimizer, self.scheduler)\n",
    "        # model.to(self.device)\n",
    "        pbar_train_dataloader = tqdm(\n",
    "            self.train_dataloader, total=len(self.train_dataloader), desc=\"Training\"\n",
    "        )\n",
    "        # Below training loop use model (not self.model) and optimizer (not self.optimizer)\n",
    "        while True:  # until meet max epoch or max patience\n",
    "            self.cur_epoch += 1\n",
    "            # dataloader = self.train_dataloader\n",
    "            pbar_train_dataloader.reset()\n",
    "            if self.cur_epoch % 3 == 0:\n",
    "                IPython.display.clear_output(wait=True)\n",
    "            print(f\"Current Epoch {self.cur_epoch}\")\n",
    "            for data in pbar_train_dataloader:\n",
    "                self.cur_step += 1\n",
    "                # CHECK EXIT CONDITION\n",
    "                if (\n",
    "                    self.cur_epoch >= self.config.max_epochs\n",
    "                    or self.exit_by_patience == True\n",
    "                ):\n",
    "                    print(\"Exit requirement reached, exiting\")\n",
    "                    self.save_checkpoint(for_last=True)\n",
    "                    return self.get_training_results()\n",
    "                # FORWARD PASS\n",
    "                model.train()\n",
    "                data = self.move_to_device(data, self.device)\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                with torch.autocast(device_type=self.device, dtype=torch.bfloat16):\n",
    "                    if type(data) is dict:\n",
    "                        output = model(**data)\n",
    "                    elif type(data) is list:\n",
    "                        output = model(*data)\n",
    "                    else:\n",
    "                        output = model(data)\n",
    "\n",
    "                    total_loss = self.extract_loss(output)\n",
    "                # BACKWARD PASS\n",
    "                total_loss.backward()\n",
    "                optimizer.step()\n",
    "                # LEARNING RATE MONITOR\n",
    "                if self.cur_step % self.config.train_log_interval == 0:\n",
    "                    for index_group in range(len(optimizer.param_groups)):\n",
    "                        lr = optimizer.param_groups[index_group][\"lr\"]\n",
    "                        self.log(f\"train/lr_group_{index_group}\", lr)\n",
    "                    print(\"Learning rate: \", lr)\n",
    "                    print(\"Total loss: \", total_loss.item())\n",
    "                scheduler.step()\n",
    "            self.on_validate_start()\n",
    "\n",
    "    def move_to_device(self, obj, device):\n",
    "        if isinstance(obj, dict):\n",
    "            d = {}\n",
    "            for k, v in obj.items():\n",
    "                d[k] = self.move_to_device(v, device)\n",
    "            return d\n",
    "        if isinstance(obj, list):\n",
    "            l = []\n",
    "            for v in obj:\n",
    "                l.append(self.move_to_device(v, device))\n",
    "            return l\n",
    "        if isinstance(obj, str):\n",
    "            return obj\n",
    "        return obj.to(device)\n",
    "\n",
    "    def on_validate_start(self):\n",
    "        metrics_dict = self.validate()\n",
    "        self.handle_checkpoint_with_patience(metrics_dict, set_patience=True)\n",
    "        self.setup_for_patience_callback()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    def validate(self):\n",
    "        validation_loader = self.val_dataloader\n",
    "        model = self.model\n",
    "\n",
    "        model.eval()\n",
    "        print(\"Evaluating\")\n",
    "        with torch.no_grad():\n",
    "            metrics = self.evaluate(validation_loader)\n",
    "            if metrics is not None:\n",
    "                history_obj = {}\n",
    "                for key in metrics:\n",
    "                    log_key = f\"val/metrics/{key}\"\n",
    "                    self.log(\n",
    "                        log_key,\n",
    "                        metrics[key],\n",
    "                    )\n",
    "                    history_obj[key] = round(metrics[key], 4)\n",
    "                    print(f\"{key}: {metrics[key]}\")\n",
    "                self.history_metrics.append(\n",
    "                    {**history_obj, \"epoch\": self.cur_epoch, \"step\": self.cur_step}\n",
    "                )\n",
    "        return metrics\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate(self, dataloader):\n",
    "        results_dict = classification_evaluate(\n",
    "            self.model,\n",
    "            self.val_dataset,\n",
    "            self.tokenizer,\n",
    "            collate_fn=self.collate_fn,\n",
    "            batch_size=self.config.batch_size,\n",
    "            num_workers=self.config.num_workers,\n",
    "            device=self.device,\n",
    "        )\n",
    "        return results_dict\n",
    "\n",
    "    def setup_for_patience_callback(self):\n",
    "        if self.current_patience > self.max_patience:\n",
    "            print(\"Early stopping\")\n",
    "            self.exit_by_patience = True\n",
    "\n",
    "    def handle_checkpoint_with_patience(\n",
    "        self, metrics: Dict[str, Any], set_patience=True\n",
    "    ):\n",
    "        reset_patience_flag = False\n",
    "        if len(self.best_metrics_values) == 0:\n",
    "            self.best_metrics_values = metrics\n",
    "            return\n",
    "        for key in self.best_metrics_values:\n",
    "            short_key = key.split(\"/\")[-1]\n",
    "            if metrics.get(short_key, -100) > self.best_metrics_values[key]:\n",
    "                self.best_metrics_values[key] = metrics[short_key]\n",
    "                self.save_checkpoint(name=f\"best_{short_key}\")\n",
    "                reset_patience_flag = True\n",
    "        if set_patience:\n",
    "            if reset_patience_flag:\n",
    "                self.current_patience = 0\n",
    "            else:\n",
    "                self.current_patience += 1\n",
    "\n",
    "    def save_checkpoint(self, name=\"last\", for_last=False):\n",
    "        model_no_ddp = self.model\n",
    "        check_point_file_path = (\n",
    "            f\"{self.checkpoint_prefix}/{name}.pt\"\n",
    "            if not for_last\n",
    "            else f\"{self.checkpoint_prefix}/last.pt\"\n",
    "        )\n",
    "        model_state_dict = model_no_ddp.state_dict()\n",
    "        save_obj = {\n",
    "            \"model\": model_state_dict,\n",
    "            \"optimizer\": self.optimizer.state_dict(),\n",
    "            \"scheduler\": self.scheduler.state_dict(),\n",
    "            \"config\": self.config,\n",
    "            \"epoch\": self.cur_epoch,\n",
    "            \"history\": self.history_metrics,\n",
    "            \"best_metrics\": self.best_metrics_values,\n",
    "            \"patience\": self.current_patience,\n",
    "        }\n",
    "        torch.save(\n",
    "            save_obj,\n",
    "            check_point_file_path,\n",
    "        )\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    def log(\n",
    "        self,\n",
    "        name: str,\n",
    "        value: Union[torch.Tensor, float, int],\n",
    "    ):\n",
    "        if type(self._logger) is not None:\n",
    "            self._logger.log({name: value, \"epoch\": self.cur_epoch}, step=self.cur_step)\n",
    "\n",
    "    def get_training_results(self):\n",
    "        history_metrics = pd.DataFrame(self.history_metrics).round(4)\n",
    "        print(history_metrics)\n",
    "        return {\n",
    "            \"best_metrics\": self.best_metrics_values,\n",
    "            \"patience\": self.current_patience,\n",
    "            \"history\": history_metrics,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TraininingConfig:\n",
    "    max_epochs: int = 50\n",
    "    lr: float = 5e-4\n",
    "    betas: Tuple[float, float] = (0.9, 0.995)\n",
    "    num_warmup_steps: int = 10\n",
    "    train_log_interval: int = 10\n",
    "    val_log_interval: int = 10\n",
    "    max_patience: int = 5\n",
    "    output_dir: str = \"checkpoints\"\n",
    "    batch_size: int = 512\n",
    "    num_workers: int = 4\n",
    "\n",
    "    def get(self, key, default=None):\n",
    "        return getattr(self, key, default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TraininingConfig(max_epochs=45, lr=0.0005, betas=(0.9, 0.995), num_warmup_steps=10, train_log_interval=10, val_log_interval=10, max_patience=7, output_dir='checkpoints', batch_size=512, num_workers=4)\n"
     ]
    }
   ],
   "source": [
    "# model = HuggingFaceModelWrapper(\"vinai/phobert-base-v2\", 3).to(\"cuda\")\n",
    "\n",
    "model = CNNClassifier(\n",
    "    word2vec_model=w2v_model,\n",
    "    input_dim=300,\n",
    "    num_filters=300,\n",
    "    filter_sizes=[3, 4, 5, 6, 7, 8],\n",
    "    output_dim=3,\n",
    "    dropout=0.2,\n",
    ").to(\"cuda\")\n",
    "\n",
    "# model = LSTMClassifier(\n",
    "#     word2vec_model=w2v_model,\n",
    "#     input_dim=300,\n",
    "#     hidden_dims=[384, 384],\n",
    "#     output_dim=3,\n",
    "#     n_layers=2,\n",
    "#     bidirectional=True,\n",
    "#     dropout=0.2,\n",
    "# ).to(\"cuda\")\n",
    "\n",
    "# model = HybridClassifer(\n",
    "#     word2vec_model=w2v_model,\n",
    "#     input_dim=300,\n",
    "#     lstm_hidden_dim=384,\n",
    "#     dropout=0.2,\n",
    "#     cnn_num_filters=300,\n",
    "#     cnn_filter_sizes=[3, 4, 5, 6, 7],\n",
    "# ).to(\"cuda\")\n",
    "\n",
    "training_config = TraininingConfig(\n",
    "    max_epochs=45, train_log_interval=10, val_log_interval=10, max_patience=7\n",
    ")\n",
    "logger = wandb.init(\n",
    "    anonymous=\"allow\",\n",
    "    project=\"<finetune><clip><har_dataset>\",\n",
    "    group=\"finetune_har_dataset\",\n",
    "    name=str(datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S-\")) + \"-finetune_clip\",\n",
    "    config=training_config,\n",
    "    notes=\"\",\n",
    "    tags=[\"<finetune>\", \"<clip>\", \"<har>\"],\n",
    ")\n",
    "IPython.display.clear_output(wait=True)\n",
    "trainer = MyTrainer(\n",
    "    config=training_config,\n",
    "    model=model,\n",
    "    train_dataset=train_table,\n",
    "    val_dataset=val_table,\n",
    "    logger=logger,\n",
    "    device=\"cuda\",\n",
    "    collate_fn=data_collator_for_cnn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Epoch 45\n",
      "Exit requirement reached, exiting\n",
      "    accuracy  macro_f1  macro_precision  macro_recall  epoch  step\n",
      "0     0.4882    0.4540           0.5261        0.4882      0     8\n",
      "1     0.4529    0.3990           0.5876        0.4529      1    17\n",
      "2     0.6000    0.5986           0.6120        0.6000      2    26\n",
      "3     0.6137    0.6133           0.6196        0.6137      3    35\n",
      "4     0.6000    0.5913           0.6335        0.6000      4    44\n",
      "5     0.6392    0.6387           0.6468        0.6392      5    53\n",
      "6     0.6373    0.6351           0.6490        0.6373      6    62\n",
      "7     0.6353    0.6343           0.6504        0.6353      7    71\n",
      "8     0.6373    0.6343           0.6583        0.6373      8    80\n",
      "9     0.6412    0.6383           0.6661        0.6412      9    89\n",
      "10    0.6510    0.6484           0.6772        0.6510     10    98\n",
      "11    0.6725    0.6719           0.6779        0.6725     11   107\n",
      "12    0.6667    0.6670           0.6792        0.6667     12   116\n",
      "13    0.6843    0.6838           0.6884        0.6843     13   125\n",
      "14    0.6725    0.6718           0.6794        0.6725     14   134\n",
      "15    0.6922    0.6927           0.6950        0.6922     15   143\n",
      "16    0.6745    0.6754           0.6832        0.6745     16   152\n",
      "17    0.6902    0.6907           0.6942        0.6902     17   161\n",
      "18    0.6941    0.6939           0.6979        0.6941     18   170\n",
      "19    0.6863    0.6862           0.6865        0.6863     19   179\n",
      "20    0.6882    0.6878           0.6880        0.6882     20   188\n",
      "21    0.6941    0.6935           0.6945        0.6941     21   197\n",
      "22    0.6902    0.6906           0.6916        0.6902     22   206\n",
      "23    0.6902    0.6908           0.6920        0.6902     23   215\n",
      "24    0.6882    0.6869           0.6944        0.6882     24   224\n",
      "25    0.6922    0.6921           0.6925        0.6922     25   233\n",
      "26    0.7000    0.7002           0.7004        0.7000     26   242\n",
      "27    0.7000    0.6998           0.7002        0.7000     27   251\n",
      "28    0.7020    0.7015           0.7032        0.7020     28   260\n",
      "29    0.7020    0.7020           0.7022        0.7020     29   269\n",
      "30    0.6961    0.6964           0.6972        0.6961     30   278\n",
      "31    0.6902    0.6900           0.6898        0.6902     31   287\n",
      "32    0.6961    0.6958           0.6985        0.6961     32   296\n",
      "33    0.6980    0.6982           0.6983        0.6980     33   305\n",
      "34    0.7000    0.7000           0.7005        0.7000     34   314\n",
      "35    0.6941    0.6939           0.6940        0.6941     35   323\n",
      "36    0.7000    0.7002           0.7005        0.7000     36   332\n",
      "37    0.7020    0.7016           0.7029        0.7020     37   341\n",
      "38    0.7000    0.7002           0.7005        0.7000     38   350\n",
      "39    0.7000    0.7000           0.7005        0.7000     39   359\n",
      "40    0.7000    0.6999           0.7010        0.7000     40   368\n",
      "41    0.7000    0.7000           0.7005        0.7000     41   377\n",
      "42    0.7000    0.7000           0.7005        0.7000     42   386\n",
      "43    0.7000    0.7000           0.7005        0.7000     43   395\n",
      "44    0.7000    0.7000           0.7005        0.7000     44   404\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'best_metrics': {'val/metrics/accuracy': 0.7019607843137254},\n",
       " 'patience': 16,\n",
       " 'history':     accuracy  macro_f1  macro_precision  macro_recall  epoch  step\n",
       " 0     0.4882    0.4540           0.5261        0.4882      0     8\n",
       " 1     0.4529    0.3990           0.5876        0.4529      1    17\n",
       " 2     0.6000    0.5986           0.6120        0.6000      2    26\n",
       " 3     0.6137    0.6133           0.6196        0.6137      3    35\n",
       " 4     0.6000    0.5913           0.6335        0.6000      4    44\n",
       " 5     0.6392    0.6387           0.6468        0.6392      5    53\n",
       " 6     0.6373    0.6351           0.6490        0.6373      6    62\n",
       " 7     0.6353    0.6343           0.6504        0.6353      7    71\n",
       " 8     0.6373    0.6343           0.6583        0.6373      8    80\n",
       " 9     0.6412    0.6383           0.6661        0.6412      9    89\n",
       " 10    0.6510    0.6484           0.6772        0.6510     10    98\n",
       " 11    0.6725    0.6719           0.6779        0.6725     11   107\n",
       " 12    0.6667    0.6670           0.6792        0.6667     12   116\n",
       " 13    0.6843    0.6838           0.6884        0.6843     13   125\n",
       " 14    0.6725    0.6718           0.6794        0.6725     14   134\n",
       " 15    0.6922    0.6927           0.6950        0.6922     15   143\n",
       " 16    0.6745    0.6754           0.6832        0.6745     16   152\n",
       " 17    0.6902    0.6907           0.6942        0.6902     17   161\n",
       " 18    0.6941    0.6939           0.6979        0.6941     18   170\n",
       " 19    0.6863    0.6862           0.6865        0.6863     19   179\n",
       " 20    0.6882    0.6878           0.6880        0.6882     20   188\n",
       " 21    0.6941    0.6935           0.6945        0.6941     21   197\n",
       " 22    0.6902    0.6906           0.6916        0.6902     22   206\n",
       " 23    0.6902    0.6908           0.6920        0.6902     23   215\n",
       " 24    0.6882    0.6869           0.6944        0.6882     24   224\n",
       " 25    0.6922    0.6921           0.6925        0.6922     25   233\n",
       " 26    0.7000    0.7002           0.7004        0.7000     26   242\n",
       " 27    0.7000    0.6998           0.7002        0.7000     27   251\n",
       " 28    0.7020    0.7015           0.7032        0.7020     28   260\n",
       " 29    0.7020    0.7020           0.7022        0.7020     29   269\n",
       " 30    0.6961    0.6964           0.6972        0.6961     30   278\n",
       " 31    0.6902    0.6900           0.6898        0.6902     31   287\n",
       " 32    0.6961    0.6958           0.6985        0.6961     32   296\n",
       " 33    0.6980    0.6982           0.6983        0.6980     33   305\n",
       " 34    0.7000    0.7000           0.7005        0.7000     34   314\n",
       " 35    0.6941    0.6939           0.6940        0.6941     35   323\n",
       " 36    0.7000    0.7002           0.7005        0.7000     36   332\n",
       " 37    0.7020    0.7016           0.7029        0.7020     37   341\n",
       " 38    0.7000    0.7002           0.7005        0.7000     38   350\n",
       " 39    0.7000    0.7000           0.7005        0.7000     39   359\n",
       " 40    0.7000    0.6999           0.7010        0.7000     40   368\n",
       " 41    0.7000    0.7000           0.7005        0.7000     41   377\n",
       " 42    0.7000    0.7000           0.7005        0.7000     42   386\n",
       " 43    0.7000    0.7000           0.7005        0.7000     43   395\n",
       " 44    0.7000    0.7000           0.7005        0.7000     44   404}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Epoch 45\n",
      "Exit requirement reached, exiting\n",
      "    accuracy  macro_f1  macro_precision  macro_recall  epoch  step\n",
      "0     0.7078    0.7084           0.7098        0.7078      0     8\n",
      "1     0.7078    0.7082           0.7091        0.7078      1    17\n",
      "2     0.6961    0.6953           0.6971        0.6961      2    26\n",
      "3     0.7000    0.7002           0.7006        0.7000      3    35\n",
      "4     0.6961    0.6956           0.6957        0.6961      4    44\n",
      "5     0.7039    0.7041           0.7046        0.7039      5    53\n",
      "6     0.6980    0.6987           0.7035        0.6980      6    62\n",
      "7     0.7059    0.7052           0.7066        0.7059      7    71\n",
      "8     0.6922    0.6925           0.6994        0.6922      8    80\n",
      "9     0.7098    0.7099           0.7113        0.7098      9    89\n",
      "10    0.6980    0.6975           0.7005        0.6980     10    98\n",
      "11    0.7078    0.7075           0.7077        0.7078     11   107\n",
      "12    0.7059    0.7064           0.7084        0.7059     12   116\n",
      "13    0.7000    0.7002           0.7070        0.7000     13   125\n",
      "14    0.7137    0.7139           0.7157        0.7137     14   134\n",
      "15    0.7000    0.6994           0.6995        0.7000     15   143\n",
      "16    0.6922    0.6929           0.6976        0.6922     16   152\n",
      "17    0.6980    0.6987           0.7022        0.6980     17   161\n",
      "18    0.6980    0.6983           0.7033        0.6980     18   170\n",
      "19    0.6980    0.6984           0.7032        0.6980     19   179\n",
      "20    0.7137    0.7142           0.7189        0.7137     20   188\n",
      "21    0.7176    0.7177           0.7195        0.7176     21   197\n",
      "22    0.7039    0.7041           0.7043        0.7039     22   206\n",
      "23    0.7039    0.7042           0.7056        0.7039     23   215\n",
      "24    0.7020    0.7017           0.7044        0.7020     24   224\n",
      "25    0.7078    0.7077           0.7091        0.7078     25   233\n",
      "26    0.7137    0.7138           0.7150        0.7137     26   242\n",
      "27    0.7039    0.7046           0.7075        0.7039     27   251\n",
      "28    0.7059    0.7058           0.7065        0.7059     28   260\n",
      "29    0.7039    0.7044           0.7078        0.7039     29   269\n",
      "30    0.7078    0.7079           0.7101        0.7078     30   278\n",
      "31    0.7157    0.7159           0.7162        0.7157     31   287\n",
      "32    0.7118    0.7119           0.7132        0.7118     32   296\n",
      "33    0.7137    0.7138           0.7143        0.7137     33   305\n",
      "34    0.7118    0.7120           0.7144        0.7118     34   314\n",
      "35    0.7137    0.7136           0.7137        0.7137     35   323\n",
      "36    0.7118    0.7122           0.7146        0.7118     36   332\n",
      "37    0.7137    0.7139           0.7141        0.7137     37   341\n",
      "38    0.7039    0.7038           0.7047        0.7039     38   350\n",
      "39    0.7118    0.7119           0.7132        0.7118     39   359\n",
      "40    0.7176    0.7180           0.7188        0.7176     40   368\n",
      "41    0.7157    0.7160           0.7167        0.7157     41   377\n",
      "42    0.7137    0.7138           0.7145        0.7137     42   386\n",
      "43    0.7137    0.7138           0.7148        0.7137     43   395\n",
      "44    0.7137    0.7138           0.7148        0.7137     44   404\n",
      "    accuracy  macro_f1  macro_precision  macro_recall  epoch  step\n",
      "0     0.7078    0.7084           0.7098        0.7078      0     8\n",
      "1     0.7078    0.7082           0.7091        0.7078      1    17\n",
      "2     0.6961    0.6953           0.6971        0.6961      2    26\n",
      "3     0.7000    0.7002           0.7006        0.7000      3    35\n",
      "4     0.6961    0.6956           0.6957        0.6961      4    44\n",
      "5     0.7039    0.7041           0.7046        0.7039      5    53\n",
      "6     0.6980    0.6987           0.7035        0.6980      6    62\n",
      "7     0.7059    0.7052           0.7066        0.7059      7    71\n",
      "8     0.6922    0.6925           0.6994        0.6922      8    80\n",
      "9     0.7098    0.7099           0.7113        0.7098      9    89\n",
      "10    0.6980    0.6975           0.7005        0.6980     10    98\n",
      "11    0.7078    0.7075           0.7077        0.7078     11   107\n",
      "12    0.7059    0.7064           0.7084        0.7059     12   116\n",
      "13    0.7000    0.7002           0.7070        0.7000     13   125\n",
      "14    0.7137    0.7139           0.7157        0.7137     14   134\n",
      "15    0.7000    0.6994           0.6995        0.7000     15   143\n",
      "16    0.6922    0.6929           0.6976        0.6922     16   152\n",
      "17    0.6980    0.6987           0.7022        0.6980     17   161\n",
      "18    0.6980    0.6983           0.7033        0.6980     18   170\n",
      "19    0.6980    0.6984           0.7032        0.6980     19   179\n",
      "20    0.7137    0.7142           0.7189        0.7137     20   188\n",
      "21    0.7176    0.7177           0.7195        0.7176     21   197\n",
      "22    0.7039    0.7041           0.7043        0.7039     22   206\n",
      "23    0.7039    0.7042           0.7056        0.7039     23   215\n",
      "24    0.7020    0.7017           0.7044        0.7020     24   224\n",
      "25    0.7078    0.7077           0.7091        0.7078     25   233\n",
      "26    0.7137    0.7138           0.7150        0.7137     26   242\n",
      "27    0.7039    0.7046           0.7075        0.7039     27   251\n",
      "28    0.7059    0.7058           0.7065        0.7059     28   260\n",
      "29    0.7039    0.7044           0.7078        0.7039     29   269\n",
      "30    0.7078    0.7079           0.7101        0.7078     30   278\n",
      "31    0.7157    0.7159           0.7162        0.7157     31   287\n",
      "32    0.7118    0.7119           0.7132        0.7118     32   296\n",
      "33    0.7137    0.7138           0.7143        0.7137     33   305\n",
      "34    0.7118    0.7120           0.7144        0.7118     34   314\n",
      "35    0.7137    0.7136           0.7137        0.7137     35   323\n",
      "36    0.7118    0.7122           0.7146        0.7118     36   332\n",
      "37    0.7137    0.7139           0.7141        0.7137     37   341\n",
      "38    0.7039    0.7038           0.7047        0.7039     38   350\n",
      "39    0.7118    0.7119           0.7132        0.7118     39   359\n",
      "40    0.7176    0.7180           0.7188        0.7176     40   368\n",
      "41    0.7157    0.7160           0.7167        0.7157     41   377\n",
      "42    0.7137    0.7138           0.7145        0.7137     42   386\n",
      "43    0.7137    0.7138           0.7148        0.7137     43   395\n",
      "44    0.7137    0.7138           0.7148        0.7137     44   404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluate model on dataset: 100%|█████████████████████████████████████████████████████████| 3/3 [00:01<00:00,  2.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate metrics: {'accuracy': 0.7095238095238096, 'macro_f1': 0.7099185109194494, 'macro_precision': 0.7106348316574992, 'macro_recall': 0.7095238095238096}\n",
      "Test results: {'accuracy': 0.7095238095238096, 'macro_f1': 0.7099185109194494, 'macro_precision': 0.7106348316574992, 'macro_recall': 0.7095238095238096}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def train_evaluate_pipeline(\n",
    "    config: DictConfig,\n",
    "    model: torch.nn.Module,\n",
    "    train_dataset: Union[torch.utils.data.Dataset, datasets.Dataset],\n",
    "    val_dataset: Union[torch.utils.data.Dataset, datasets.Dataset],\n",
    "    test_dataset: Union[torch.utils.data.Dataset, datasets.Dataset],\n",
    "    collate_fn: Callable,\n",
    "    tokenizer: Optional[transformers.PreTrainedTokenizer] = None,\n",
    "    device: str = \"cuda\",\n",
    "    num_workers: int = 2,\n",
    "    logger: Optional[wandb.sdk.wandb_run.Run] = None,\n",
    "):\n",
    "    trainer = MyTrainer(\n",
    "        config=config,\n",
    "        model=model,\n",
    "        train_dataset=train_dataset,\n",
    "        val_dataset=val_dataset,\n",
    "        logger=logger,\n",
    "        device=device,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "    trainer.train()\n",
    "    trainer.save_checkpoint(for_last=True)\n",
    "    results = trainer.get_training_results()\n",
    "\n",
    "    test_results = classification_evaluate(\n",
    "        model,\n",
    "        test_dataset,\n",
    "        tokenizer,\n",
    "        collate_fn=collate_fn,\n",
    "        batch_size=config.batch_size,\n",
    "        num_workers=num_workers,\n",
    "        device=device,\n",
    "    )\n",
    "    print(f\"Test results: {test_results}\")\n",
    "    return test_results\n",
    "\n",
    "\n",
    "evaluation_results = train_evaluate_pipeline(\n",
    "    config=training_config,\n",
    "    model=model,\n",
    "    train_dataset=train_table,\n",
    "    val_dataset=val_table,\n",
    "    test_dataset=test_table,\n",
    "    collate_fn=data_collator_for_cnn,\n",
    "    tokenizer=None,\n",
    "    device=\"cuda\",\n",
    "    num_workers=2,\n",
    "    logger=logger,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.7095238095238096,\n",
       " 'macro_f1': 0.7099185109194494,\n",
       " 'macro_precision': 0.7106348316574992,\n",
       " 'macro_recall': 0.7095238095238096}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    accuracy  macro_f1  macro_precision  macro_recall  epoch  step\n",
      "0     0.4882    0.4540           0.5261        0.4882      0     8\n",
      "1     0.4529    0.3990           0.5876        0.4529      1    17\n",
      "2     0.6000    0.5986           0.6120        0.6000      2    26\n",
      "3     0.6137    0.6133           0.6196        0.6137      3    35\n",
      "4     0.6000    0.5913           0.6335        0.6000      4    44\n",
      "5     0.6392    0.6387           0.6468        0.6392      5    53\n",
      "6     0.6373    0.6351           0.6490        0.6373      6    62\n",
      "7     0.6353    0.6343           0.6504        0.6353      7    71\n",
      "8     0.6373    0.6343           0.6583        0.6373      8    80\n",
      "9     0.6412    0.6383           0.6661        0.6412      9    89\n",
      "10    0.6510    0.6484           0.6772        0.6510     10    98\n",
      "11    0.6725    0.6719           0.6779        0.6725     11   107\n",
      "12    0.6667    0.6670           0.6792        0.6667     12   116\n",
      "13    0.6843    0.6838           0.6884        0.6843     13   125\n",
      "14    0.6725    0.6718           0.6794        0.6725     14   134\n",
      "15    0.6922    0.6927           0.6950        0.6922     15   143\n",
      "16    0.6745    0.6754           0.6832        0.6745     16   152\n",
      "17    0.6902    0.6907           0.6942        0.6902     17   161\n",
      "18    0.6941    0.6939           0.6979        0.6941     18   170\n",
      "19    0.6863    0.6862           0.6865        0.6863     19   179\n",
      "20    0.6882    0.6878           0.6880        0.6882     20   188\n",
      "21    0.6941    0.6935           0.6945        0.6941     21   197\n",
      "22    0.6902    0.6906           0.6916        0.6902     22   206\n",
      "23    0.6902    0.6908           0.6920        0.6902     23   215\n",
      "24    0.6882    0.6869           0.6944        0.6882     24   224\n",
      "25    0.6922    0.6921           0.6925        0.6922     25   233\n",
      "26    0.7000    0.7002           0.7004        0.7000     26   242\n",
      "27    0.7000    0.6998           0.7002        0.7000     27   251\n",
      "28    0.7020    0.7015           0.7032        0.7020     28   260\n",
      "29    0.7020    0.7020           0.7022        0.7020     29   269\n",
      "30    0.6961    0.6964           0.6972        0.6961     30   278\n",
      "31    0.6902    0.6900           0.6898        0.6902     31   287\n",
      "32    0.6961    0.6958           0.6985        0.6961     32   296\n",
      "33    0.6980    0.6982           0.6983        0.6980     33   305\n",
      "34    0.7000    0.7000           0.7005        0.7000     34   314\n",
      "35    0.6941    0.6939           0.6940        0.6941     35   323\n",
      "36    0.7000    0.7002           0.7005        0.7000     36   332\n",
      "37    0.7020    0.7016           0.7029        0.7020     37   341\n",
      "38    0.7000    0.7002           0.7005        0.7000     38   350\n",
      "39    0.7000    0.7000           0.7005        0.7000     39   359\n",
      "40    0.7000    0.6999           0.7010        0.7000     40   368\n",
      "41    0.7000    0.7000           0.7005        0.7000     41   377\n",
      "42    0.7000    0.7000           0.7005        0.7000     42   386\n",
      "43    0.7000    0.7000           0.7005        0.7000     43   395\n",
      "44    0.7000    0.7000           0.7005        0.7000     44   404\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'best_metrics': {'val/metrics/accuracy': 0.7019607843137254},\n",
       " 'patience': 16,\n",
       " 'history':     accuracy  macro_f1  macro_precision  macro_recall  epoch  step\n",
       " 0     0.4882    0.4540           0.5261        0.4882      0     8\n",
       " 1     0.4529    0.3990           0.5876        0.4529      1    17\n",
       " 2     0.6000    0.5986           0.6120        0.6000      2    26\n",
       " 3     0.6137    0.6133           0.6196        0.6137      3    35\n",
       " 4     0.6000    0.5913           0.6335        0.6000      4    44\n",
       " 5     0.6392    0.6387           0.6468        0.6392      5    53\n",
       " 6     0.6373    0.6351           0.6490        0.6373      6    62\n",
       " 7     0.6353    0.6343           0.6504        0.6353      7    71\n",
       " 8     0.6373    0.6343           0.6583        0.6373      8    80\n",
       " 9     0.6412    0.6383           0.6661        0.6412      9    89\n",
       " 10    0.6510    0.6484           0.6772        0.6510     10    98\n",
       " 11    0.6725    0.6719           0.6779        0.6725     11   107\n",
       " 12    0.6667    0.6670           0.6792        0.6667     12   116\n",
       " 13    0.6843    0.6838           0.6884        0.6843     13   125\n",
       " 14    0.6725    0.6718           0.6794        0.6725     14   134\n",
       " 15    0.6922    0.6927           0.6950        0.6922     15   143\n",
       " 16    0.6745    0.6754           0.6832        0.6745     16   152\n",
       " 17    0.6902    0.6907           0.6942        0.6902     17   161\n",
       " 18    0.6941    0.6939           0.6979        0.6941     18   170\n",
       " 19    0.6863    0.6862           0.6865        0.6863     19   179\n",
       " 20    0.6882    0.6878           0.6880        0.6882     20   188\n",
       " 21    0.6941    0.6935           0.6945        0.6941     21   197\n",
       " 22    0.6902    0.6906           0.6916        0.6902     22   206\n",
       " 23    0.6902    0.6908           0.6920        0.6902     23   215\n",
       " 24    0.6882    0.6869           0.6944        0.6882     24   224\n",
       " 25    0.6922    0.6921           0.6925        0.6922     25   233\n",
       " 26    0.7000    0.7002           0.7004        0.7000     26   242\n",
       " 27    0.7000    0.6998           0.7002        0.7000     27   251\n",
       " 28    0.7020    0.7015           0.7032        0.7020     28   260\n",
       " 29    0.7020    0.7020           0.7022        0.7020     29   269\n",
       " 30    0.6961    0.6964           0.6972        0.6961     30   278\n",
       " 31    0.6902    0.6900           0.6898        0.6902     31   287\n",
       " 32    0.6961    0.6958           0.6985        0.6961     32   296\n",
       " 33    0.6980    0.6982           0.6983        0.6980     33   305\n",
       " 34    0.7000    0.7000           0.7005        0.7000     34   314\n",
       " 35    0.6941    0.6939           0.6940        0.6941     35   323\n",
       " 36    0.7000    0.7002           0.7005        0.7000     36   332\n",
       " 37    0.7020    0.7016           0.7029        0.7020     37   341\n",
       " 38    0.7000    0.7002           0.7005        0.7000     38   350\n",
       " 39    0.7000    0.7000           0.7005        0.7000     39   359\n",
       " 40    0.7000    0.6999           0.7010        0.7000     40   368\n",
       " 41    0.7000    0.7000           0.7005        0.7000     41   377\n",
       " 42    0.7000    0.7000           0.7005        0.7000     42   386\n",
       " 43    0.7000    0.7000           0.7005        0.7000     43   395\n",
       " 44    0.7000    0.7000           0.7005        0.7000     44   404}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.get_training_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
